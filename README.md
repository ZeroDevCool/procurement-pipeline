<p align="center">
  <img src="docs/banner.png" alt="Project Banner" width="600"/>
</p>

# üìä Procurement Analytics Data Engineering Project

Este proyecto demuestra un pipeline de ingenier√≠a de datos completo para analizar el ciclo de compras (SP, OC, entrega) utilizando herramientas modernas como BigQuery, Terraform, Airflow y Dataform.

---

## üõ† Tecnolog√≠as utilizadas

- Google Cloud Platform (BigQuery, Cloud Functions, Cloud Storage)
- Terraform
- Dataform
- Apache Airflow (Cloud Composer)
- Power BI / Looker Studio

---

## üîÑ Flujo de datos

1. **Ingesta**: Archivos CSV simulados de SAP (`EBAN`, `EKPO`, `EKET`) se cargan a Cloud Storage.
2. **Procesamiento**: Cloud Function sube los datos a BigQuery.
3. **Modelado**: Dataform transforma y modela los datos para anal√≠tica.
4. **Orquestaci√≥n**: Airflow ejecuta el pipeline de forma automatizada.
5. **Visualizaci√≥n**: Dashboard en Power BI.

---

## üìÅ Estructura del Proyecto

```
data-engineering-procurement-analytics/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ terraform/
‚îú‚îÄ‚îÄ dataform/
‚îú‚îÄ‚îÄ dags/
‚îú‚îÄ‚îÄ cloudfunctions/
‚îÇ   ‚îî‚îÄ‚îÄ upload_to_gcs/
‚îÇ       ‚îú‚îÄ‚îÄ main.py
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ docs/
‚îî‚îÄ‚îÄ dashboard/
```

---

## üöÄ ¬øC√≥mo ejecutarlo?

1. Crea un proyecto GCP y habilita BigQuery, Cloud Storage y Cloud Functions.
2. Despliega la infraestructura con Terraform.
3. Sube archivos de datos a GCS.
4. Ejecuta los DAGs de Airflow.
5. Visualiza los resultados en Power BI o Looker Studio.

---

## üì• Ingesta de archivos CSV a Google Cloud Storage (GCS)

Para automatizar la carga de los archivos `eban.csv`, `ekpo.csv` y `eket.csv` desde la carpeta `/data` local hacia un bucket en GCS, se cre√≥ una **Cloud Function** en Python que utiliza la librer√≠a oficial `google-cloud-storage`.

### üìÅ Estructura del m√≥dulo

```
cloudfunctions/
‚îî‚îÄ‚îÄ upload_to_gcs/
    ‚îú‚îÄ‚îÄ main.py
    ‚îî‚îÄ‚îÄ requirements.txt
```

### ‚öôÔ∏è Requisitos previos

1. Tener el SDK de Google Cloud instalado:  
   üëâ https://cloud.google.com/sdk/docs/install

2. Autenticarse en tu cuenta de GCP:

```bash
gcloud auth application-default login
```

3. Instalar dependencias:

```bash
pip install -r cloudfunctions/upload_to_gcs/requirements.txt
```

4. Ejecutar la funci√≥n:

```bash
python cloudfunctions/upload_to_gcs/main.py
```

---

## ü™£ Crear un bucket en Google Cloud Storage (GCS)

Puedes hacerlo de dos formas:

### ‚úÖ Opci√≥n 1: Usar Terraform

Archivo `terraform/main.tf`:

```hcl
provider "google" {
  project = "TU_ID_PROYECTO"
  region  = "us-central1"
}

resource "google_storage_bucket" "procurement_data_bucket" {
  name     = "bucket-procurement-analytics"
  location = "US"
}
```

### ‚úÖ Ejecuci√≥n en terminal:

```bash
cd terraform
terraform init
terraform apply
```

### ‚úÖ Opci√≥n 2: Desde la consola GCP

1. Ve a https://console.cloud.google.com/storage/browser
2. Clic en "Crear bucket"
3. Nombre: `bucket-procurement-analytics`
4. Regi√≥n: `US`
5. Opciones por defecto ‚Üí Crear

---

## üß± Modelado de Datos con Dataform (BigQuery)

Este proyecto utiliza **Dataform** para transformar los datos brutos cargados en BigQuery en tablas limpias, normalizadas y validadas, listas para an√°lisis de KPIs.

### üìÅ Estructura del m√≥dulo

```
dataform/
‚îú‚îÄ‚îÄ dataform.json
‚îî‚îÄ‚îÄ definitions/
    ‚îú‚îÄ‚îÄ models/
    ‚îÇ   ‚îú‚îÄ‚îÄ compras.sqlx
    ‚îÇ   ‚îú‚îÄ‚îÄ resumen_oc.sqlx
    ‚îÇ   ‚îî‚îÄ‚îÄ calidad_datos.assertion.sqlx
    ‚îî‚îÄ‚îÄ includes/
        ‚îî‚îÄ‚îÄ helpers.js
```

### ‚öôÔ∏è Requisitos

1. Tener Node.js y npm
2. Instalar el CLI de Dataform:

```bash
npm install -g @dataform/cli
```

3. Autenticarse en GCP:

```bash
gcloud auth application-default login
```

### üîß Configuraci√≥n b√°sica (`dataform.json`)

```json
{
  "warehouse": "bigquery",
  "defaultSchema": "procurement_analytics",
  "assertionSchema": "procurement_analytics_assertions",
  "defaultDatabase": "TU_PROYECTO_ID",
  "gcloudProjectId": "TU_PROYECTO_ID",
  "defaultDataset": "procurement_analytics",
  "assertionDataset": "procurement_analytics_assertions"
}
```

> Reemplaza `TU_PROYECTO_ID` por el ID de tu proyecto en GCP.

### ‚ñ∂Ô∏è Ejecuci√≥n de modelos

```bash
cd dataform
dataform install
dataform run
```

---

## üìä Ejemplo de KPIs

- Valor estimado vs real por proveedor
- SP/OC por estado
- Tiempo promedio del ciclo de compra
- Alertas por desviaci√≥n presupuestaria

---

**Autor**: Jonathan Tejo  
**Licencia**: MIT

## üîÅ Orquestaci√≥n con Airflow

Para automatizar el pipeline completo (desde la ingesta hasta el modelado), se implement√≥ un **DAG en Apache Airflow** que ejecuta las tareas clave de este proyecto.

### üìÑ Archivo del DAG

```
dags/
‚îî‚îÄ‚îÄ ingest_procurement_dag.py
```

### üìã ¬øQu√© hace este DAG?

- Registra el inicio del proceso (`start`)
- Ejecuta el comando `dataform run` para transformar los datos en BigQuery
- Ejecuta una validaci√≥n final (`validate`)
- Registra el fin del proceso (`end`)

### üß± Estructura del DAG

```python
start >> dataform_run >> validate >> end
```

### üìÖ Frecuencia de ejecuci√≥n

Este DAG est√° configurado para ejecutarse una vez al d√≠a (`@daily`), pero puedes ajustar `schedule_interval` seg√∫n tus necesidades.

### ‚ñ∂Ô∏è C√≥mo ejecutar (modo local)

1. Copia el archivo `ingest_procurement_dag.py` a tu carpeta `dags/` de Airflow.
2. Aseg√∫rate de que tu entorno Airflow tenga acceso a:
   - Dataform instalado globalmente (`npm install -g @dataform/cli`)
   - Autenticaci√≥n de GCP configurada (`gcloud auth application-default login`)
3. Inicia Airflow webserver y scheduler:

```bash
airflow webserver &
airflow scheduler
```

4. Activa el DAG desde la interfaz web de Airflow (http://localhost:8080).

---
